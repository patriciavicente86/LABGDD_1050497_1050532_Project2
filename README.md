# NYC Taxi ML/DL Pipeline â€” Lambda, Kappa & Predictive Analytics

> **LABGDD Project 2**: Big Data pipeline with **Machine Learning**, **Deep Learning (GPU)**, and comprehensive **testing framework**

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](https://python.org)
[![PySpark](https://img.shields.io/badge/PySpark-3.5%2B-orange)](https://spark.apache.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15%2B-ff6f00)](https://tensorflow.org)
[![Tests](https://img.shields.io/badge/Tests-Pytest-green)](https://pytest.org)

---

## ğŸ¯ Project Overview

This project extends **Project 1** (Lambda vs Kappa architecture comparison) with:

- âœ… **Machine Learning** (Spark MLlib): Random Forest, GBT, Linear Regression for demand forecasting
- âœ… **Deep Learning** (TensorFlow + GPU): LSTM time series forecasting  
- âœ… **Testing Framework**: Unit + Integration + Data Quality tests (**30% of grade**)
- âœ… **Performance Benchmarks**: CPU vs GPU, ML vs DL comparisons
- âœ… **Topics Integration**: Parallel Computing, GPU Computing, Cloud Architecture

---

## ğŸ“ Project Structure

```
LABGDD_1050497_1050532_Project2/
â”œâ”€â”€ src/                       # Core pipeline (inherited from Project 1)
â”‚   â”œâ”€â”€ clean_to_silver.py     # Bronze â†’ Silver transformation
â”‚   â”œâ”€â”€ features_to_gold.py    # Silver â†’ Gold feature engineering
â”‚   â”œâ”€â”€ kappa_driver.py        # Streaming pipeline (Kappa)
â”‚   â”œâ”€â”€ lambda_driver.py       # Batch pipeline (Lambda)
â”‚   â”œâ”€â”€ compare_lambda_kappa.py# Lambda vs Kappa comparison
â”‚   â”œâ”€â”€ metrics.py             # Pipeline metrics and reports
â”‚   â”œâ”€â”€ figures.py             # Visualization generation
â”‚   â”œâ”€â”€ ingest_bronze.py       # Raw data ingestion
â”‚   â””â”€â”€ probe_stream.py        # Streaming data probe
â”œâ”€â”€ ml/                        # Machine Learning module
â”‚   â””â”€â”€ demand_forecasting.py  # Spark MLlib models
â”œâ”€â”€ dl/                        # Deep Learning module
â”‚   â””â”€â”€ lstm_forecaster.py     # TensorFlow LSTM model
â”œâ”€â”€ tests/                     # Testing framework (30% grade!)
â”‚   â”œâ”€â”€ conftest.py            # Pytest configuration
â”‚   â”œâ”€â”€ test_pipeline.py       # Unit tests
â”‚   â””â”€â”€ test_integration.py    # Integration tests
â”œâ”€â”€ benchmarks/                # Performance benchmarks
â”‚   â””â”€â”€ performance_benchmark.py
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â””â”€â”€ NYC_Taxi_Report.ipynb
â”œâ”€â”€ data/                      # Raw data (parquet files)
â”‚   â”œâ”€â”€ yellow/2024/
â”‚   â””â”€â”€ green/2024/
â”œâ”€â”€ lake/                      # Data lake (Bronze/Silver/Gold)
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ docker/                    # Container configuration
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ env/                       # Configuration
â”‚   â””â”€â”€ config.yaml            # Unified config (paths, ML, DL, tests)
â”œâ”€â”€ make/                      # Makefile modules
â”‚   â”œâ”€â”€ pipeline.mk            # Pipeline targets
â”‚   â”œâ”€â”€ ml.mk                  # ML/DL/test targets
â”‚   â”œâ”€â”€ ingest.mk
â”‚   â”œâ”€â”€ bench.mk
â”‚   â””â”€â”€ validate.mk
â”œâ”€â”€ Makefile                   # Main makefile
â”œâ”€â”€ pytest.ini                 # Pytest configuration
â””â”€â”€ README.md
```

---

## ğŸ—ï¸ Architecture

```
NYC Taxi Data (37M+ trips)
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚  Bronze â”‚  Raw data ingestion
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚  Silver â”‚  Data cleaning + validation
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚   Gold  â”‚  Analytics-ready features
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Lambda â”‚           â”‚  Kappa  â”‚
â”‚ (Batch)â”‚           â”‚(Stream) â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Spark  â”‚           â”‚  LSTM   â”‚
â”‚ MLlib  â”‚           â”‚  (GPU)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Setup

### Prerequisites
- Python 3.10+
- PySpark 3.5+
- TensorFlow 2.15+ (optional, for DL)
- CUDA 11.8+ (optional, for GPU acceleration)

### Installation

```bash
cd LABGDD_1050497_1050532_Project2

# Install dependencies
pip install -r docker/requirements.txt
```

### Verify Installation

```bash
# Check Spark
python -c "from pyspark.sql import SparkSession; print('Spark OK')"

# Check TensorFlow (optional)
python -c "import tensorflow as tf; print('GPU:', tf.config.list_physical_devices('GPU'))"
```

---

## ğŸš€ Usage

### Quick Start (Data Already Processed)

If you have the `lake/` folder with processed data:

```bash
# Check pipeline metrics
make metrics

# Generate figures
make figures

# Run ML models
make ml

# Run DL model (GPU recommended)
make dl

# Run all tests
make test
```

### Full Pipeline (From Scratch)

```bash
# 1. Ingest raw data to Bronze
make ingest

# 2. Clean and transform to Silver
make silver

# 3. Feature engineering to Gold
make gold

# 4. Train ML models
make ml

# 5. Train DL model
make dl

# 6. Run tests
make test
```

### Lambda vs Kappa Comparison

```bash
# Start Kappa streaming (in separate terminal)
make kappa_start

# Seed data to streaming path
make kappa_seed

# Compare results
make compare
```

---

## ğŸ¤– Machine Learning

### Models Implemented

| Model | Algorithm | Library |
|-------|-----------|---------|
| Linear Regression | OLS | Spark MLlib |
| Random Forest | Ensemble | Spark MLlib |
| Gradient Boosted Trees | Boosting | Spark MLlib |

### Features Used

- **Temporal**: hour, day_of_week, month, day_of_month
- **Lag Features**: prev_hour_demand, prev_2hour_demand, prev_day_demand
- **Trip Metrics**: avg_distance, avg_fare, avg_duration

### Run ML Pipeline

```bash
make ml
```

Or programmatically:

```python
from ml.demand_forecasting import DemandForecaster

forecaster = DemandForecaster()
results = forecaster.run_pipeline()

for name, metrics in results.items():
    print(f"{name}: RMSE={metrics['rmse']:.2f}, RÂ²={metrics['r2']:.4f}")
```

---

## ğŸ§  Deep Learning

### LSTM Architecture

```
Input (24 hours) â†’ LSTM(128) â†’ Dropout(0.2) 
                â†’ LSTM(64)  â†’ Dropout(0.2) 
                â†’ Dense(32) â†’ Dense(1) â†’ Output
```

### Configuration (from `env/config.yaml`)

```yaml
dl:
  use_gpu: true
  lookback: 24
  epochs: 50
  batch_size: 32
  lstm_units_1: 128
  lstm_units_2: 64
  dropout_rate: 0.2
  early_stopping_patience: 10
```

### Run DL Pipeline

```bash
make dl
```

Or programmatically:

```python
from dl.lstm_forecaster import LSTMDemandForecaster

forecaster = LSTMDemandForecaster(use_gpu=True)
metrics = forecaster.run_pipeline(zone_id=237, lookback=24)

print(f"LSTM: RMSE={metrics['rmse']:.2f}, RÂ²={metrics['r2']:.4f}")
```

---

## âœ… Testing (30% of Grade!)

### Test Categories

| Category | File | Description |
|----------|------|-------------|
| Unit Tests | `test_pipeline.py` | Schema, cleaning, features |
| Integration Tests | `test_integration.py` | End-to-end pipeline |
| Data Quality | `test_pipeline.py` | Completeness, consistency |

### Run Tests

```bash
# All tests
make test

# Unit tests only
make test-unit

# Integration tests only
make test-integration

# With coverage report
make test-coverage
```

### Test Coverage

```bash
pytest --cov=src --cov=ml --cov=dl --cov-report=html
```

---

## âš™ï¸ Configuration

All configuration is centralized in `env/config.yaml`:

```yaml
# Paths (supports both flat and nested format)
paths:
  lake: "lake"
  bronze: "lake/bronze"
  silver: "lake/silver"
  gold: "lake/gold"
  data: "data"

# Data Quality
data_quality:
  min_trip_duration: 1
  max_trip_duration: 180
  min_trip_distance: 0.1
  max_trip_distance: 100

# Spark
spark:
  master: "local[*]"
  driver_memory: "4g"

# ML Configuration
ml:
  test_split: 0.2
  random_seed: 42

# DL Configuration  
dl:
  use_gpu: true
  epochs: 50
  lookback: 24
```

---

## ğŸ“Š Results

### Data Pipeline

| Layer | Records | Retention |
|-------|---------|-----------|
| Bronze | 38.7M | 100% |
| Silver | 37.0M | 95.4% |
| Gold | 37.0M | 100% |

### Model Performance (Expected)

| Model | RMSE | RÂ² | Training Time |
|-------|------|-----|---------------|
| Linear Regression | ~22 | ~0.78 | ~30 sec |
| Random Forest | ~17 | ~0.88 | ~5 min |
| Gradient Boosting | ~15 | ~0.91 | ~8 min |
| LSTM (GPU) | ~13 | ~0.93 | ~2.5 min |

---

## ğŸ“š Topics Coverage

| Course Topic | Implementation |
|--------------|----------------|
| Parallel Computing | âœ… Spark distributed processing |
| GPU Computing | âœ… TensorFlow GPU acceleration |
| Cloud Computing | âœ… Docker containerization |
| Hadoop/Spark | âœ… Lambda + Kappa architectures |
| Machine Learning | âœ… Spark MLlib (RF, GBT, LR) |
| Deep Learning | âœ… LSTM time series forecasting |

---

## ğŸ“ AI Assistance Disclosure

This project was developed with assistance from:
- GitHub Copilot (code completion and suggestions)
- ChatGPT (architecture planning, documentation)

All code has been reviewed, tested, and validated by the team.

---

## ğŸ‘¥ Authors

**Students**: 1050497, 1050532  
**Course**: LABGDD - Big Data Laboratory  
**Institution**: ISEP - MEI Data Engineering  
**Year**: 2024/2025

---

## ğŸ“„ License

Academic project for educational purposes only.

---

## ğŸ”— Quick Reference

```bash
# Common commands
make metrics          # View data statistics
make figures          # Generate visualizations
make ml               # Train ML models
make dl               # Train DL model
make test             # Run all tests
make test-coverage    # Tests with coverage

# Pipeline commands
make silver           # Bronze â†’ Silver
make gold             # Silver â†’ Gold
make kappa_start      # Start streaming

# Cleanup
make clean            # Remove checkpoints
make clean-models     # Remove trained models
```
